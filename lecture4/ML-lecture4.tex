\documentclass[12pt]{beamer}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{ifluatex}
\usefonttheme[onlymath]{serif}
\usepackage{svg}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{mathtools}

\definecolor{beamer@darkgreen}{rgb}{0,0.6,0}
\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{title}{fg=black,bg=beamer@darkgreen}
\setbeamercolor{frametitle}{fg=black,bg=beamer@darkgreen}
\setbeamercolor{background canvas}{parent=normal text}
\setbeamertemplate{footline}[frame number]
\usepackage[english,russian]{babel}
\usepackage{graphicx}
\usepackage{listings}

\author{Катя Тузова}
\title{Машинное обучение}
\subtitle{Лекция 4. Методы кластеризации}
\date{}

\begin{document}
\frame{\titlepage}


\begin{frame}\frametitle{Разбор летучки}
Какая в худшем случае сложность поиска в kd-дереве из n элементов? \\
Какой случай является “худшим”?
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$O(n)$\\
На каждом шаге сфера радиуса $R$ пересекает разделяющую поверхность. 
\end{frame}


\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/kd-nn}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/kd-nn1}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/kd-nn2}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Что важнее для алгоритма классификации: точность или полнота? Почему?
\end{frame}


\begin{frame}\frametitle{Разбор летучки}
$$\operatorname{Precision} = \frac{TP}{TP + FP}$$\\
\begin{itemize}
\item[--] $TP$ --- это количество элементов, которые классификатор верно отнёс к классу $c$,
\item[--] $FP$ --- количество элементов, которые классификатор неверно отнёс к классу $c$,
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$$\operatorname{Precision} = \frac{TP}{TP + FP}$$\\
\vspace{5mm}
Характеризует, сколько полученных от классификатора положительных ответов являются правильными. Чем больше точность, тем меньше число ложных попаданий.
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$$\operatorname{Precision} = \frac{TP}{TP + FP}$$\\
\vspace{5mm}
Что остается неизвестным?
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$$\operatorname{Precision} = \frac{TP}{TP + FP}$$\\
\vspace{5mm}
Мера точности не дает представления о том, все ли правильные ответы вернул классификатор.
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$$\operatorname{Recall} = \frac{TP}{TP + FN}$$\\
\begin{itemize}
\item[--] $TP$ --- это количество элементов, которые классификатор верно отнёс к классу $c$,
\item[--] $FN$ --- количество элементов, которые классификатор неверно отнёс к классу, отличному от $c$.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$$\operatorname{Recall} = \frac{TP}{TP + FN}$$\\
\vspace{5mm}
Мера полноты характеризует способность классификатора «угадывать» как можно большее число положительных ответов из ожидаемых. 
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/precision-recall}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/precision-recall-1}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/precision-recall-2}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
\begin{figure}[htbp]
\centering
\includegraphics[height=170pt]{images/precision-recall-3}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
$F_1 = 2 \frac{P R}{P + R}$\\
\vspace{5mm}
F-мера представляет собой гармоническое среднее между точностью и полнотой. Она стремится к нулю, если точность или полнота стремится к нулю.
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Кратко опишите основную идею locality-sensitive hashing.
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Найти такую хэш-функцию $h$, что:\\
\begin{itemize}
\item[--] Если объекты $C_1$ и $C_2$ похожи, то с большой вероятностью $h(C1) == h(C2)$\\
\item[--] Иначе -- с большой вероятностью $h(C1) \neq h(C2)$
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Приведите пример кластерной структуры на которой алгоритм Ланса-Уильямса с
параметрами, соответствующими групповому среднему, будет работать плохо.
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Сгущения\\
\vspace{5mm}
\begin{figure}[htbp]
  \includegraphics[height=130pt, keepaspectratio = true]{images/cluster1}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Ленты\\
\vspace{5mm}
\begin{figure}[htbp]
  \includegraphics[height=130pt, keepaspectratio = true]{images/cluster2}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
С центром\\
\vspace{5mm}
\begin{figure}[htbp]
  \includegraphics[height=130pt, keepaspectratio = true]{images/cluster3}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
С перемычками\\
\vspace{5mm}
\begin{figure}[htbp]
  \includegraphics[height=130pt, keepaspectratio = true]{images/cluster4}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
На фоне\\
\vspace{5mm}
\begin{figure}[htbp]
  \includegraphics[height=130pt, keepaspectratio = true]{images/cluster5}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Перекрывающиеся\\
\vspace{5mm}
\begin{figure}[htbp]
  \includegraphics[height=130pt, keepaspectratio = true]{images/cluster6}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разбор летучки}
Соответствует ли дендрограмма кластеризации точек a, b, c, d алгоритмом Ланса-Уильямса с параметрами, соответствующими групповому среднему? \\
Если нет, то приведите вариант, который считаете правильным.
\end{frame}

\begin{frame}\frametitle{Дендрограмма}
Может ли так случиться, что дендрограмма имеет самопересечения?\\
\vspace{5mm}
\end{frame}

\begin{frame}\frametitle{Свойство монотонности}
Кластеризация монотонна, если на каждом шаге расстояние $\rho$ между объединяемыми кластерами не уменьшается.\\
$\rho_2 \leq \rho_3 \leq \dots \leq \rho_l$
\end{frame}

\begin{frame}\frametitle{Постановка задачи кластеризации}
Кластеризация -- задача разделения объектов одной природы на несколько групп так, чтобы объекты в одной группе обладали одним и тем же свойством.\\
\vspace{5mm}
Кластеризация -- это обучение без учителя.
\end{frame}

\begin{frame}\frametitle{Постановка задачи кластеризации}
$X$ -- пространство объектов\\
$\rho: X \times X \rightarrow [0, \infty)$ -- функция расстояния между объектами\\
\vspace{5mm}
Найти:\\
$Y$ -- множество кластеров \\
$a: X \rightarrow Y$ -- алгоритм кластеризации
\vspace{5mm}

\end{frame}

\begin{frame}\frametitle{Степени свободы в постановке задачи}
	\begin{itemize}
		\item[--] Критерий качества кластеризации
		\item[--] Число кластеров неизвестно заранее
		\item[--] Результат кластеризации существенно зависит от метрики
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Цели кластеризации}
	\begin{itemize}
		\item[--] Сократить объём хранимых данных
		\item[--] Выделить нетипичные объекты
		\item[--] Упростить дальнейшую обработку данных
		\item[--] Построить иерархию множества объектов				
	\end{itemize}
\end{frame}

\begin{frame}\frametitle{Оценка качества кластеризации}
\begin{itemize}
\item[--] Минимизировать среднее внутрикластерное расстояние\\
\vspace{5mm}
${\frac{\sum\limits_{a(x_i) = a(x_j)} \rho(x_i, x_j)}{\sum\limits_{a(x_i) = a(x_j)} 1} \rightarrow \min}$
\item[--] Максимизировать среднее межкластерное расстояние\\
\vspace{5mm}
${\frac{\sum\limits_{a(x_i) \neq a(x_j)} \rho(x_i, x_j)}{\sum\limits_{a(x_i) \neq a(x_j)} 1} \rightarrow \max}$
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Методы кластеризации}
\begin{itemize}
\item[--] Иерархические
\item[--] Графовые 
\item[--] Статистические 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Графовые алгоритмы}
Какие есть две очевидные идеи?
\end{frame}

\begin{frame}\frametitle{Графовые алгоритмы}
Очевидные:\\
\begin{itemize}
\item[--] Выделение связных компонент
\item[--] Минимальное покрывающее дерево
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Выделение связных компонент}
\begin{itemize}
\item[--] Рисуем полный граф с весами, равными расстоянию между объектами
\item[--] Выбираем лимит расстояния $r$ и выкидываем все ребра длиннее $r$
\item[--] Компоненты связности полученного графа -- наши кластеры
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Выделение связных компонент}
Как искать компоненты связности?
\end{frame}

\begin{frame}\frametitle{Минимальное покрывающее дерево}
Минимальное остовное дерево -- дерево, содержащее все вершины графа и имеющее минимальный суммарный вес ребер.\\
\vspace{5mm}
Как найти?
\end{frame}

\begin{frame}\frametitle{Минимальное покрывающее дерево}
Как использовать минимальное остовное дерево для разбиения на кластеры?
\end{frame}
\begin{frame}\frametitle{Минимальное покрывающее дерево}
Строим минимальное остовное дерево, а потом выкидываем из него ребра максимального веса.\\
\vspace{5mm}
Сколько ребер выбросим -- столько кластеров получим.
\end{frame}

\begin{frame}\frametitle{Статистические алгоритмы}
\end{frame}

\begin{frame}\frametitle{Алгоритм FOREL}
Идея:\\
\begin{itemize}
\item[--] Выделить все точки выборки $x_i$, попадающие внутрь сферы $\rho(x_i, x_0) \leq R$
\item[--] Перенести $x_0$ в центр тяжести выделенных точек
\item[--] Повторять пока $x_0$ не стабилизируется
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Алгоритм FOREL}
Input: X, R\\
${U = X, C = 0}$\\\vspace{2mm}
while ${U \neq 0}$:\\
\hspace{5mm} выбрать случайную точку $x_0$\\
\vspace{2mm}
\hspace{5mm} Повторять пока $x_0$ не стабилизируется:\\
\vspace{2mm}
\hspace{10mm} ${c = \left\{ x \in X \vert \rho(x, x_0) < R \right\}}$ \\
\vspace{2mm}
\hspace{10mm} $x_0 = \frac{1}{\vert c \vert} \sum\limits_{x \in c} x$\\
\vspace{2mm}
\hspace{5mm} ${U = U \setminus c}$, ${C = C \cup \left\{ c \right\}}$
\end{frame}

\begin{frame}\frametitle{Алгоритм FOREL}

\begin{itemize}
\item[+] Наглядность
\item[+] Сходимость
\item[--] Зависимость от выбора $x_0$
\item[--] Плохо работает, если изначальная выборка плохо делится на кластеры
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
Идея:\\  минимизировать меру ошибки\\
\vspace{5mm}${E(X, C) = \sum\limits_{i = 1}^n \Vert x_i -\mu_i \Vert^2}$\\
\vspace{5mm}
$\mu_i$ -- ближайший к $x_i$ центр кластера
\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
Инициализировать центры $k$ кластеров \\
\vspace{2mm}
Пока $c_i$ не перестанет меняться:\\
\hspace{5mm} $c_i = \arg\min\limits_{c \in C} \rho(x_i, \mu_c)$ \hspace{5mm} $i = 1,\dots, l$\\
\vspace{2mm}\hspace{5mm} ${\mu_c = \frac{\sum\limits_{c_i = c} f_j(x_i)}{\sum\limits_{c_i = c} 1} }$ \hspace{10mm} $j = 1,\dots, n$, $c \in C$\\
\vspace{2mm}
$\mu_c$ -- новое положение центров кластеров\\
$c_i$ -- принадлежность $x_i$ к кластеру\\
$\rho(x_i, \mu_c)$ -- расстояние от $x_i$ до центра кластера $\mu_c$
\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/k-means-1}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/k-means-2}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/k-means-3}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/k-means-4}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Метод $k$-средних}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/k-means-5}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Особенности метода $k$-средних}
\begin{itemize}
\item[--] Чувствительность к начальному выбору $\mu_c$
\item[--] Необходимость задавать $k$
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Чувствительность к начальному выбору $\mu_c$}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/local_min2}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Чувствительность к начальному выбору $\mu_c$}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/local_min4}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Чувствительность к начальному выбору $\mu_c$}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/local_min6}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Чувствительность к начальному выбору $\mu_c$}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/local_min7}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Необходимость задавать $k$}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/k_means_k}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Устранение недостатков}
\end{frame}

\begin{frame}\frametitle{Устранение недостатков}
\begin{itemize}
\item[--] Несколько случайных кластеризаций
\item[--] Постепенное наращивание числа $k$
\item[--] Использование k-means++
\end{itemize}
\end{frame}

\begin{frame}\frametitle{k-means++}
\begin{itemize}
\item[--] Выбрать первый центроид случайным образом
\item[--] Для каждой точки найти значение квадрата расстояния до ближайшего центроида.
\item[--] Выбрать из этих точек следующий центроид так, чтобы вероятность выбора точки была пропорциональна вычисленному для неё квадрату расстояния
\end{itemize}
\end{frame}

\begin{frame}\frametitle{X-means}
Идея:\\
\begin{itemize}
\item[--] Получать на вход не k, а диапазон, в котором может находиться k.
\item[--] Запустить k-means на самом маленьком значении из диапазона.
\item[--] Разбивать пополам полученные кластеры и проверять, не улучшилась ли кластеризация.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{X-means}

\begin{figure}[htbp]
  \includegraphics[height=140pt, keepaspectratio = true]{images/x-means}  
    \includegraphics[height=140pt, keepaspectratio = true]{images/x-means-1}
\end{figure}
\end{frame}

\begin{frame}\frametitle{X-means}
Как проверить, что кластеризация улучшилась?
\end{frame}

\begin{frame}\frametitle{Байесовский информационный критерий}
$BIC = E(X, C)  + k d \log(n)$\\
\vspace{5mm}
$E$ -- внутрикластерная сумма квадратов расстояний\\
$k$ -- количество кластеров\\
$d$ -- длина вектора фич\\
$n$ -- количество объектов в выборке\\

\end{frame}

\begin{frame}\frametitle{Недостатки k-means}
\end{frame}

\begin{frame}\frametitle{"Не сферические данные"}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/non_spherical-1}  
\end{figure}
\end{frame}	

\begin{frame}\frametitle{"Не сферические данные"}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/non_spherical-2}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разноразмерные кластеры}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/different_sizes-1}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{Разноразмерные кластеры}
\begin{figure}[htbp]
  \includegraphics[height=180pt, keepaspectratio = true]{images/different_sizes-2}  
\end{figure}
\end{frame}

\begin{frame}\frametitle{На следующей лекции}
\begin{itemize}
\item[--] Линейные методы классификации
\item[--] Минимизация эмпирического риска
\item[--] Метод градиентного спуска
\item[--] Принцип максимума правдоподобия
\end{itemize}
\end{frame}

\end{document}
