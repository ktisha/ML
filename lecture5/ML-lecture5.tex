\documentclass[12pt]{beamer}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{ifluatex}
\usefonttheme[onlymath]{serif}
\usepackage{svg}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{mathtools}
\setbeamertemplate{footline}[frame number]
\definecolor{beamer@darkgreen}{rgb}{0,0.6,0}
\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{title}{fg=black,bg=beamer@darkgreen}
\setbeamercolor{frametitle}{fg=black,bg=beamer@darkgreen}
\setbeamercolor{background canvas}{parent=normal text}

\usepackage[english,russian]{babel}
\usepackage{graphicx}
\usepackage{listings}
\DeclareMathOperator{\sign}{sign}

\author{Катя Тузова}
\title{Машинное обучение}
\subtitle{Лекция 5. Линейные методы классификации.}
\date{}

\begin{document}	
\frame{\titlepage}

\begin{frame}\frametitle{Разбор летучки}
\end{frame}


\begin{frame}\frametitle{Постановка задачи}
$X = \mathbb{R}^n$, ${Y = \left\{ -1, + 1\right\}}$\\
${X^l = (x_i, y_i)_{i = 1}^l}$ -- обучающая выборка\\
\vspace{5mm}Найти:\\
$(n-1)$-мерную гиперплоскость, которая разделяет данные как можно лучше.
\\ \vspace{5mm}
Как можно лучше -- это как?

\end{frame}

\begin{frame}\frametitle{Пример}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/example}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Постановка задачи}
Как можно лучше:\\
Два разделенных класса должны лежать как можно дальше от разделяющей гиперплоскости.\\
\end{frame}

%\begin{frame}\frametitle{Опорная гиперплоскость}
%\end{frame}

%\begin{frame}\frametitle{Опорная гиперплоскость}
%Гиперплоскость называется опорной для множества точек
%$X$, если все точки из $X$ лежат по одну сторону от этой гиперплоскости.\\\vspace{5mm}
%${f(x,w) = \langle x, w\rangle - w_0 = 0}$\\
%\vspace{5mm}
%Как посчитать расстояние от точки до гиперплоскости?
%\end{frame}

%\begin{frame}\frametitle{Опорная гиперплоскость}
%Гиперплоскость называется опорной для множества точек
%$X$, если все точки из $X$ лежат по одну сторону от этой гиперплоскости.\\\vspace{5mm}
%${f(x,w) = \langle x, w\rangle - w_0 = 0}$\\
%\vspace{5mm}
%Расстояние от точки до гиперплоскости:
%$\frac{\vert f(x,w) \vert}{\Vert w \Vert}$
%\end{frame}

\begin{frame}\frametitle{Вопрос}
Как построить такую гиперплоскость?
\end{frame}

\begin{frame}\frametitle{Пример}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/example}   
\end{figure}
\end{frame}


\begin{frame}\frametitle{Выпуклая оболочка}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/example1}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Выпуклая оболочка}
Выпуклой оболочкой множества $X$ называется наименьшее выпуклое множество, содержащее $X$.\\
\vspace{5mm}
Выпуклое множество — множество, содержащее вместе с любыми двумя точками соединяющий их отрезок.
\end{frame}

\begin{frame}\frametitle{Выпуклая оболочка}
\begin{figure}[htbp]
	\begin{minipage}{.5\textwidth}
	  \includegraphics[height=130pt, keepaspectratio = true]{images/convex} \\
	  \centering Выпуклая оболочка
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
		\includegraphics[height=130pt, keepaspectratio = true]{images/non-convex}   \\
		\centering Невыпуклая оболочка
	\end{minipage}%
\end{figure}
\end{frame}

\begin{frame}\frametitle{Выпуклая оболочка}
Найти две ближайшие точки в выпуклых
оболочках данных, а затем провести разделяющую
гиперплоскость через середину отрезка.
\end{frame}

\begin{frame}\frametitle{Выпуклая оболочка}
$\min\limits_w \Vert c - d \Vert^2$, где $c = \sum\limits_{y_i = 1} w_ix_i$,  $d = \sum\limits_{y_i = -1} w_ix_i$\\
\vspace{5mm}
$\sum\limits_{y_i = 1}w_i = \sum\limits_{y_i = -1}w_i = 1$, $w_i \geq 0$\\
\vspace{5mm}
Задачу можно решать общими оптимизационными алгоритмами.
\end{frame}

\begin{frame}\frametitle{Построение разделяющей поверхности}
\end{frame}


\begin{frame}\frametitle{Построение разделяющей поверхности}
${X^l = (x_i,y_i)_{i = 1}^l}$ -- обучающая выборка\\ 
${Y=\left\{-1,+1\right\}}$\\
\vspace{5mm}
Задача:\\
Построить алгоритм классификации ${a(x,w) = \sign g(x,w)}$\\\vspace{5mm}
${g(x,w)}$ -- разделяющая функция\\
$w$ -- вектор параметров, $w \in \mathbb{R}^l$
\end{frame}


\begin{frame}\frametitle{Линейный классификатор}
$f_j: X \rightarrow \mathbb{R}$, $j = 1,\dots, n$\\
\vspace{3mm}
$a(x, w) = sign(\sum\limits_{j=1}^n w_jf_j(x) - w_0)$\\
$w_j \in \mathbb{R}$ -- веса признаков\\
\vspace{7mm}
Введём константный признак $f_0 \equiv -1$:\\
$a(x, w) = sign(\sum\limits_{j=0}^n w_jf_j(x))$\\
\end{frame}

\begin{frame}\frametitle{Линейный классификатор}
$a(x, w) = sign(\sum\limits_{j=1}^n w_jf_j(x))$\\

\vspace{3mm}
Перейдём к векторной записи:\\
$\mathbf{x} = (f_1, f_2, \dots, f_n)$\\
\vspace{3mm}
$a(\mathbf{x}, \mathbf{w}) = sign(\langle \mathbf{w}, \mathbf{x}\rangle)$\\
\end{frame}


\begin{frame}\frametitle{Линейный классификатор}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/neuron}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Линейный классификатор}
Как подбирать $w_j$?
\end{frame}

\begin{frame}\frametitle{Построение разделяющей поверхности}
Как оценить качество классификации?
\end{frame}

\begin{frame}\frametitle{Определение отступа}
${g(\mathbf{x}, \mathbf{w}) = \langle \mathbf{w}, \mathbf{x}\rangle = 0}$ -- разделяющая поверхность\\
$M_i(\mathbf{w}) = \langle \mathbf{w}, \mathbf{x_i}\rangle y_i$ -- отступ объекта $\mathbf{x_i}$\\
${M_i(\mathbf{w})<0}$ $\Rightarrow$ алгоритм $a(\mathbf{x},\mathbf{w})$ ошибается на $\mathbf{x_i}$
\end{frame}

\begin{frame}\frametitle{Минимизация эмпирического риска}
Число ошибок на обучающей выборке:\\
\vspace{5mm}
${Q(\mathbf{w}) = \sum\limits_{i=1}^l \left[ M_i(\mathbf{w}) < 0 \right] \rightarrow \min\limits_{\mathbf{w}} }$\\
\vspace{3mm}
${Q(\mathbf{w})}$ -- функционал качества\\
\vspace{5mm}
Но так мы учитываем только результат классификации и теряем информацию насколько ${i}$-й объект был надежен.\\
\vspace{3mm}
Как сохранить информацию?

\end{frame}

\begin{frame}\frametitle{Отступ}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/margin1}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Отступ}
\begin{figure}[htbp]
  \includegraphics[height=190pt, keepaspectratio = true]{images/margin2}   
\end{figure}
\end{frame}


\begin{frame}\frametitle{Функция $[M<0]$}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/l1}   
\end{figure}
\end{frame}


\begin{frame}\frametitle{Минимизация эмпирического риска}
${Q(\mathbf{w}) = \sum\limits_{i=1}^l \left[ M_i(\mathbf{w}) < 0 \right] \leq}$\\ \vspace{3mm}
${\leq \widetilde{Q}(\mathbf{w}) = \sum\limits_{i=1}^l \mathcal{L}(M_i(\mathbf{w})) \rightarrow \min\limits_{\mathbf{w}} }$\\\vspace{3mm}
$\mathcal{L}$ -- функция потерь, невозрастающая, неотрицательная.\\ 
\end{frame}

\begin{frame}\frametitle{Примеры $\mathcal{L}$}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/l}   
\end{figure}
\begin{enumerate}
\item $V(M) = (1-M)_+$ -- кусочно-линейная
\item $L(M) = \log_2(1+e^{-M})$ -- логарифмическая
\item $S(M) = 2(1+e^M)^{-1}$ -- сигмоидная
\end{enumerate}
\end{frame}

\begin{frame}\frametitle{Градиент}
Что такое градиент?
\end{frame}

\begin{frame}\frametitle{Градиент}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/gradient}   
\end{figure}
\end{frame}

%Численный метод оптимизации
\begin{frame}\frametitle{Метод градиентного спуска}
Input: $\eta$ -- градиентный шаг (темп обучения)\\
Output: $w_0, w_1, \dots, w_n$\\
\vspace{3mm}	
Инициализировать: $w_j$, $j=0,\dots, n$\\
Повторить пока $\mathbf{w}$ не стабилизируются:\\
\hspace{10mm} $\mathbf{w} =  \mathbf{w} - \eta \bigtriangledown Q(\mathbf{w})$\\

\vspace{10mm}
$\bigtriangledown Q(\mathbf{w}) = (\frac{\partial Q(\mathbf{w})}{\partial w_j})_{j=0}^n$\\
\end{frame}

\begin{frame}\frametitle{Градиентный спуск}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/gradient_descent}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Метод градиентного спуска}
Случай двух признаков.\\
Input: $\eta$ -- градиентный шаг (темп обучения)\\
Output: $w_0, w_1$\\
\vspace{3mm}	
Инициализировать: $w_0$, $w_1$\\
Повторить пока $w_0$ и $w_1$ не стабилизируются:\\
\hspace{10mm} $tmp_0 =  w_0 - \eta \frac{\partial Q(w)}{\partial w_0}$\\
\hspace{10mm} $tmp_1 =  w_1 - \eta \frac{\partial Q(w)}{\partial w_1}$\\
\hspace{10mm} $w_0 = tmp_0$\\
\hspace{10mm} $w_1 = tmp_1$
\end{frame}

\begin{frame}\frametitle{Метод градиентного спуска}
Почему важно обновить $w_0$ и $w_1$ одновременно?
\end{frame}

\begin{frame}\frametitle{Градиент функционала качества $Q$}
$\bigtriangledown Q(\mathbf{w}) = (\frac{\partial Q(\mathbf{w})}{\partial w_j})_{j=0}^n = \sum\limits_{i=1}^l \mathcal{L}'(\langle \mathbf{w}, \mathbf{x_i} \rangle y_i) \mathbf{x_i} y_i$\\
\end{frame}

\begin{frame}\frametitle{Маленький градиентный шаг}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/learning_rate_small}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Большой градиентный шаг}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/learning_rate_large}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Выбор величины шага}
\begin{itemize}
\item[--] Для выпуклых функций:\\
$\eta_t \rightarrow 0$\\
$\sum\limits_{t = 1}^\infty \eta_t = \infty$\\
$\sum\limits_{t = 1}^\infty \eta_t^2 < \infty$\\

\item[--] Метод скорейшего градиентного спуска:\\
$Q(w - \eta \bigtriangledown Q(w)) \rightarrow \min\limits_{\eta}$
\item[--] Пробные случайные шаги
\end{itemize}
\end{frame}

\begin{frame}\frametitle{В чем проблема?}

\end{frame}

\begin{frame}\frametitle{В чем проблема?}
$\mathbf{w} =  \mathbf{w} - \eta \sum\limits_{i=1}^l \mathcal{L}'(\langle \mathbf{w}, \mathbf{x_i} \rangle y_i)\mathbf{x_i}y_i$\\
\vspace{5mm}
\end{frame}

\begin{frame}\frametitle{Метод стохастического градиента}
$\mathbf{w} =  \mathbf{w} - \eta \sum\limits_{i=1}^l \mathcal{L}'(\langle \mathbf{w}, \mathbf{x_i}\rangle y_i)\mathbf{x_i}y_i$\\
\vspace{5mm}
Давайте брать $(x_i, y_i)$ по одному и сразу обновлять вектор весов
\end{frame}


\begin{frame}\frametitle{Алгоритм}
Input: $X^l$, $\eta$, $\lambda$\\
Output: $w_0, w_1, \dots, w_n$\\
\vspace{3mm}
Инициализировать: $w_j$, $j=0,\dots, n$\\
\hspace{35mm} ${Q}(\mathbf{w}) = \sum\limits_{i=1}^l \mathcal{L}(\langle \mathbf{w}, \mathbf{x_i} \rangle y_i)$\\
Повторить пока $Q$ и/или $w$ не стабилизируются:\\
\hspace{5mm} выбрать $x_i$ из $X^l$ случайным образом\\
\hspace{5mm} Потеря: $\varepsilon_i = \mathcal{L}(\langle \mathbf{w}, \mathbf{x_i} \rangle y_i)$\\
\hspace{5mm} Градиентный шаг: $w =  w - \eta \mathcal{L}'(\langle \mathbf{w}, \mathbf{x_i}\rangle y_i)\mathbf{x_i}y_i$\\
\hspace{5mm} Оценить $Q = (1-\lambda)Q + \lambda \varepsilon_i$
\end{frame}


\begin{frame}\frametitle{Градиентный спуск}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/gradient_descent2}   
\end{figure}
\end{frame}


\begin{frame}\frametitle{Учет ошибки $\varepsilon_i$ в алгоритме}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/l2}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Вопрос}
Чего не хватает?
\end{frame}

\begin{frame}\frametitle{Вопрос}
Как выбрать параметр $\lambda$?
\end{frame}

\begin{frame}\frametitle{Эмпирическое правило для выбора $\lambda$}
$\lambda = \frac{1}{k}$, где $k$ -- количество объектов, по которым хотим усреднять функционал.
\end{frame}

\begin{frame}\frametitle{Вопрос}
Что значит -- "пока $Q$ и/или $w$ не стабилизируются"?
\end{frame}

\begin{frame}\frametitle{Зависимость $Q$ от номера итерации}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/stochastic_gradient}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Зависимость $Q$ от номера итерации}
\begin{figure}[htbp]
  \includegraphics[height=160pt, keepaspectratio = true]{images/stochastic_gradient1}   
\end{figure}
\end{frame}

\begin{frame}\frametitle{Актуальные вопросы}
\begin{itemize}
\item[--] Инициализация весов
\item[--] Порядок предъявления объектов
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Инициализация весов}

\end{frame}

\begin{frame}\frametitle{Инициализация весов}
\begin{itemize}
\item[--] $w_j = 0$, j = 1, \dots, n
\item[--] $w_j = random(-\frac{1}{2n}, \frac{1}{2n})$ -- небольшие случайные значения
\item[--] $w_j = \frac{\langle y, f_j \rangle}{\langle f_j, f_j \rangle}$, $f_j = (f_j(x_i))_{i = 1}^n$
\item[--] Обучение по небольшой случайной подвыборке объектов
\item[--] Многократный запуск из разных случайных начальных приближений
\end{itemize}

\end{frame}

\begin{frame}\frametitle{Порядок предъявления объектов}

\end{frame}

\begin{frame}\frametitle{Порядок предъявления объектов}
\begin{itemize}
\item[--] Попеременно брать объекты из разных классов
\item[--] Чаще брать те объекты, на которых была допущена большая ошибка
\item[--] Вообще не брать "хорошие" объекты с $M_i > \mu_+$
\item[--] Вообще не брать выбросы с $M_i < \mu_-$
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Плюсы и минусы}
\begin{itemize}
\item[+] Легко реализовать
\item[+] Легко обобщить на разные $f$, $\mathcal{L}$
\item[+] Не обязательно брать все элементы выборки для обучения
\end{itemize}
\begin{itemize}
\item[--] Возможна медленная сходимость
\item[--] Застревание в локальных минимумах
\item[--] Подбор параметров
\item[--] Проблема переобучения
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Проблема переобучения}
Почему случается переобучение?
\end{frame}

\begin{frame}\frametitle{Проблема переобучения}
\begin{itemize}
\item[--] Слишком мало объектов
\item[--] Слишком много признаков
\item[--] Линейная зависимость признаков
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Как заподозрить?}
\begin{itemize}
\item[--] Слишком большие веса $\Vert \mathbf{w} \Vert$
\item[--] Неустойчивость $a(\mathbf{x},\mathbf{w})$
\item[--] Плохое качество классификации на контрольных данных
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Решение}
\begin{itemize}
\item[--] Сокращение весов
\item[--] Ранняя остановка алгоритма
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Сокращение весов}
Как можно сокращать веса?
\end{frame}

\begin{frame}\frametitle{Сокращение весов}
Штраф за увеличение нормы вектора весов:\\
$Q_{\tau} = Q + \frac{\tau}{2}\Vert \mathbf{w} \Vert^2 \rightarrow \min\limits_{\mathbf{w}}$\\
\vspace{5mm}
Градиент:\\
$\bigtriangledown Q_{\tau} = \bigtriangledown Q + \tau \mathbf{w}$\\
\vspace{5mm}
Градиентный шаг:\\
$\mathbf{w} = \mathbf{w}(1-\eta \tau) - \eta \bigtriangledown Q(\mathbf{w})$
\end{frame}

\begin{frame}\frametitle{Вопрос}
Как подобрать $\tau$?
\end{frame}

\begin{frame}\frametitle{Как подобрать $\tau$?}
\begin{itemize}
\item[--] Скользящий контроль
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Степени свободы}

\end{frame}

\begin{frame}\frametitle{Степени свободы}
\begin{itemize}
\item[--] Вид разделяющей поверхности
\item[--] Вид аппроксимации функции потерь $\mathcal{L}(M)$
\item[--] Вид регуляризатора
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Резюме лекции}
\end{frame}

\begin{frame}\frametitle{На следующей лекции}
\begin{itemize}
\item[--] Метод опорных векторов
\end{itemize}
\end{frame}
\end{document}
